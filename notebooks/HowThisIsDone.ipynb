{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a1b047",
   "metadata": {},
   "source": [
    "# Генерация текста с помощью LSTM-сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a1a01e",
   "metadata": {},
   "source": [
    "Сеть способна выучить распределение символов в последовательностях\n",
    "\n",
    "\n",
    "Датасет формируем проходясь окном по текстовому корпусу, задача сети - предсказывать следующий символ на основании нескольких предыдущих.\n",
    "Данный подход можно улучшить, используя только отдельные предложения с паддингами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9ed051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfd0ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7670e60",
   "metadata": {},
   "source": [
    "### 0. Получение данных для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ed477",
   "metadata": {},
   "source": [
    "Для обучения используется датасет российских новостей, который был сохранён в файл `text_corpus.parquet` со следующими параметрами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fbdf599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_parquet(\"text_corpus.parquet\", engine=\"pyarrow\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ede6899",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"text_corpus.parquet\", engine=\"pyarrow\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23a15fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88f6c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# после обучения токенизатора можно уменьшить тренировочную выборку\n",
    "# но нужно не забыть обновить переменную corpus\n",
    "data = data.sample(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43565f10",
   "metadata": {},
   "source": [
    "### 1. Вспомогательные функции:\n",
    "+ Визуализация процесса обучения\n",
    "    + Сможем посмотреть, как меняется качество с течением времени.\n",
    "+ Коллбек ModelCheckpoint\n",
    "    + Процесс обучения LSTM сетей достаточно длительный. Будет обидно, если из-за непредвиденного сбоя потеряется прогресс за многие часы обучения.\n",
    "+ Колбек динамической подстройки размера батча и learning rate\n",
    "    + Подстраивать LR это уже стандартная практика, а я хочу ещё и размер батча менять: предположу, что большой батч позволит дать некое \"обобщённое\" представление о распределении токенов, а маленький батч улучшит \"грамотность\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6075ecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b619d2ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# весь текст одной \"портянкой\", чтобы заранее оценить, какие символы могут нам попадаться\n",
    "# raw_text = \" \".join(data.text)\n",
    "# chars = sorted(list(set(raw_text)))\n",
    "# chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b205ce5",
   "metadata": {},
   "source": [
    "### 3. Предобработка и создание датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b543a",
   "metadata": {},
   "source": [
    "Для тренировки LSTM модели понадобится немного поработать с форматами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbba1257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ccdc753",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \" \\n\".join(data.text.to_list()).lower()\n",
    "# Хочу отделить всю пунктуацию от слов пробелом\n",
    "corpus = \" \".join(re.findall(r\"[\\w']+|[.,!?;\\n]\", corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e3d87fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = 800\n",
    "max_sequence_length = 80\n",
    "n_samples = 8000000 # сколько тренировочных последовательностей потом сгенерируем из корпуса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6174aa87",
   "metadata": {},
   "source": [
    "#### 3.1 Токенизация BPE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467888c5",
   "metadata": {},
   "source": [
    "BPE токенизация посредством yttm эффективна, но потребуется поработать с файлом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08b2d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import youtokentome as yttm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e9648c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_model_path = \"bpe.yttm\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ff9bb4",
   "metadata": {},
   "source": [
    "##### 3.2 Создаём токенизатор BPE и обучаем его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae1a2a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bpe_tokenizer_from_scratch(corpus, train_data_path=\"yttm_train_data.txt\"):\n",
    "    with open(train_data_path, \"w\") as _file:\n",
    "        _file.writelines(corpus)\n",
    "    # Training model\n",
    "    # (data, model, vocab_size, coverage, n_threads=-1, pad_id=0, unk_id=1, bos_id=2, eos_id=3)\n",
    "    return yttm.BPE.train(data=train_data_path, vocab_size=total_words, model=bpe_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3397405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model\n",
    "bpe = create_bpe_tokenizer_from_scratch(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84aead7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "bpe = yttm.BPE(model=bpe_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "939a22da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD> <UNK> <BOS> <EOS> ▁ о е и а н т с р в л к п д м у я ы г з б , ь ч й . х ж ' ц ю ш ф щ э ъ ? ё ! ; _ ▁п ▁с ▁в ▁, ст ни ра ро но ре на ▁о ко то ▁. ▁и ▁по го не де те ли ва ▁м за ны ▁на ль ка ри та ле ла ▁д во ве ▁б ти ци ▁со ви ▁ч ки ло ▁у ▁за ▁' да ть ен ми ▁а ▁не ▁ко сс ▁пре ет ру ся ди ▁про н\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(bpe.vocab())[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "189a25b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode(self, \n",
    "#     sentences, \n",
    "#     output_type=yttm.OutputType.ID, \n",
    "#     bos=False, \n",
    "#     eos=False, \n",
    "#     reverse=False, \n",
    "#     dropout_prob=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9425a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_corpus = np.array(bpe.encode(corpus))\n",
    "\n",
    "# sequences = sequence[:-(len(sequence)%max_sequence_length)].reshape((len(sequence)//max_sequence_length, max_sequence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaa7343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fbc1360",
   "metadata": {},
   "source": [
    "## 4. Модель"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cccada6",
   "metadata": {},
   "source": [
    "В качестве модели будет применяться LSTM сеть с двумя слоями LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eab4fc",
   "metadata": {},
   "source": [
    "TODO\n",
    "+ Gradient clipping\n",
    "+ More layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f47d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Model)\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_size=max_input_length,\n",
    "            num_classes=total_words,\n",
    "            hidden_dim=64,\n",
    "            num_layers=2,\n",
    "            batch_size=128,\n",
    "                ):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0)\n",
    "        # Bi-LSTM\n",
    "        # Forward and backward\n",
    "        self.lstm_cell_forward = nn.LSTMCell(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
    "        self.lstm_cell_backward = nn.LSTMCell(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
    "        # LSTM layer\n",
    "        self.lstm_cell = nn.LSTMCell(self.hidden_dim * 2, self.hidden_dim * 2, batch_first=True)\n",
    "        \n",
    "#         self.lstm = nn.LSTM(\n",
    "#             max_input_length,  # input_size – The number of expected features in the input x\n",
    "#             hidden_dim, # hidden_size – The number of features in the hidden state h\n",
    "#             num_layers, # num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "#             # bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "#             batch_first=True# batch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n",
    "#             # dropout – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "#             bidirectional=True# bidirectional – If True, becomes a bidirectional LSTM. Default: False\n",
    "#             # proj_size – If > 0, will use LSTM with projections of corresponding size. Default: 0\n",
    "#         )\n",
    "        \n",
    "        self.linear = nn.Linear(self.hidden_dim * 2, self.num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Bi-LSTM\n",
    "        # hs = [batch_size x hidden_size]\n",
    "        # cs = [batch_size x hidden_size]\n",
    "        hs_forward = torch.zeros(X.size(0), self.hidden_dim)\n",
    "        cs_forward = torch.zeros(X.size(0), self.hidden_dim)\n",
    "        hs_backward = torch.zeros(X.size(0), self.hidden_dim)\n",
    "        cs_backward = torch.zeros(X.size(0), self.hidden_dim)\n",
    "\n",
    "        # LSTM\n",
    "        # hs = [batch_size x (hidden_size * 2)]\n",
    "        # cs = [batch_size x (hidden_size * 2)]\n",
    "        hs_lstm = torch.zeros(X.size(0), self.hidden_dim * 2)\n",
    "        cs_lstm = torch.zeros(X.size(0), self.hidden_dim * 2)\n",
    "\n",
    "        # Weights initialization\n",
    "        torch.nn.init.kaiming_normal_(hs_forward)\n",
    "        torch.nn.init.kaiming_normal_(cs_forward)\n",
    "        torch.nn.init.kaiming_normal_(hs_backward)\n",
    "        torch.nn.init.kaiming_normal_(cs_backward)\n",
    "        torch.nn.init.kaiming_normal_(hs_lstm)\n",
    "        torch.nn.init.kaiming_normal_(cs_lstm)\n",
    "\n",
    "        # From idx to embedding\n",
    "        out = self.embedding(X)\n",
    "\n",
    "        # Prepare the shape for LSTM Cells\n",
    "#         out = out.view(self.sequence_len, X.size(0), -1)\n",
    "\n",
    "        forward = []\n",
    "        backward = []\n",
    "\n",
    "        # Unfolding Bi-LSTM\n",
    "        # Forward\n",
    "        for i in range(self.input_size):\n",
    "            hs_forward, cs_forward = self.lstm_cell_forward(out[i], (hs_forward, cs_forward))\n",
    "            hs_forward = self.dropout(hs_forward)\n",
    "            cs_forward = self.dropout(cs_forward)\n",
    "            forward.append(hs_forward)\n",
    "\n",
    "         # Backward\n",
    "        for i in reversed(range(self.sequence_len)):\n",
    "            hs_backward, cs_backward = self.lstm_cell_backward(out[i], (hs_backward, cs_backward))\n",
    "            hs_backward = self.dropout(hs_backward)\n",
    "            cs_backward = self.dropout(cs_backward)\n",
    "            backward.append(hs_backward)\n",
    "\n",
    "         # LSTM\n",
    "        for fwd, bwd in zip(forward, backward):\n",
    "            input_tensor = torch.cat((fwd, bwd), 1)\n",
    "            hs_lstm, cs_lstm = self.lstm_cell(input_tensor, (hs_lstm, cs_lstm))\n",
    "\n",
    "         # Last hidden state is passed through a linear layer\n",
    "        out = self.linear(hs_lstm)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3f40c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, args):\n",
    "  \n",
    "      # Model initialization\n",
    "      model = TextGenerator(args, self.vocab_size)\n",
    "\n",
    "      # Optimizer initialization\n",
    "      optimizer = optim.RMSprop(model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "      # Defining number of batches\n",
    "      num_batches = int(len(self.sequences) / self.batch_size)\n",
    "\n",
    "      # Set model in training mode\n",
    "      model.train()\n",
    "\n",
    "      # Training pahse\n",
    "      for epoch in range(self.num_epochs):\n",
    "\n",
    "            # Mini batches\n",
    "            for i in range(num_batches):\n",
    "\n",
    "                  # Batch definition\n",
    "                try:\n",
    "                    x_batch = self.sequences[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "                    y_batch = self.targets[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "                except:\n",
    "                    x_batch = self.sequences[i * self.batch_size :]\n",
    "                    y_batch = self.targets[i * self.batch_size :]\n",
    "\n",
    "                # Convert numpy array into torch tensors\n",
    "                x = torch.from_numpy(x_batch).type(torch.LongTensor)\n",
    "                y = torch.from_numpy(y_batch).type(torch.LongTensor)\n",
    "\n",
    "                # Feed the model\n",
    "                y_pred = model(x)\n",
    "\n",
    "                # Loss calculation\n",
    "                loss = F.cross_entropy(y_pred, y.squeeze())\n",
    "\n",
    "                # Clean gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Calculate gradientes\n",
    "                loss.backward()\n",
    "\n",
    "                # Updated parameters\n",
    "                optimizer.step()\n",
    "\n",
    "                print(\"Epoch: %d ,  loss: %.5f \" % (epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b95fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "torch.save(model.state_dict(), 'weights/textGenerator_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77838f82",
   "metadata": {},
   "source": [
    "## 5. Инференс полученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a1e7124e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "приветствие затянулось на несколько вродители,\n",
      "им зае призиденторта согорами в сосесуми рроведе с сосрийской ке просева об пнобо с общела он вобороднтое влодедать в инсотия чесьдае в сосрий сазорунотти в соссий сакали проведа пода саков\n"
     ]
    }
   ],
   "source": [
    "def generator(model, sequences, idx_to_char, n_chars):\n",
    "  \n",
    "    # Set the model in evalulation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Define the softmax function\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    # Randomly is selected the index from the set of sequences\n",
    "    start = np.random.randint(0, len(sequences)-1)\n",
    "\n",
    "    # The pattern is defined given the random idx\n",
    "    pattern = sequences[start]\n",
    "\n",
    "    # By making use of the dictionaries, it is printed the pattern\n",
    "    print(\"\\nPattern: \\n\")\n",
    "    print(''.join([idx_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "    # In full_prediction we will save the complete prediction\n",
    "    full_prediction = pattern.copy()\n",
    "\n",
    "    # The prediction starts, it is going to be predicted a given\n",
    "    # number of characters\n",
    "    for i in range(n_chars):\n",
    "\n",
    "        # The numpy patterns is transformed into a tesor-type and reshaped\n",
    "        pattern = torch.from_numpy(pattern).type(torch.LongTensor)\n",
    "        pattern = pattern.view(1,-1)\n",
    "\n",
    "        # Make a prediction given the pattern\n",
    "        prediction = model(pattern)\n",
    "        # It is applied the softmax function to the predicted tensor\n",
    "        prediction = softmax(prediction)\n",
    "\n",
    "        # The prediction tensor is transformed into a numpy array\n",
    "        prediction = prediction.squeeze().detach().numpy()\n",
    "        # It is taken the idx with the highest probability\n",
    "        arg_max = np.argmax(prediction)\n",
    "\n",
    "        # The current pattern tensor is transformed into numpy array\n",
    "        pattern = pattern.squeeze().detach().numpy()\n",
    "        # The window is sliced 1 character to the right\n",
    "        pattern = pattern[1:]\n",
    "        # The new pattern is composed by the \"old\" pattern + the predicted character\n",
    "        pattern = np.append(pattern, arg_max)\n",
    "\n",
    "        # The full prediction is saved\n",
    "        full_prediction = np.append(full_prediction, arg_max)\n",
    "\n",
    "    print(\"Prediction: \\n\")\n",
    "    print(''.join([idx_to_char[value] for value in full_prediction]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3b71dfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD>']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "11e6c404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"не планирует передислоцировать наблюдательные пункты в идлибской зоне деэскалации , при этом турция продолжит отправлять военных и бронетехнику в этот район 'в целях защиты мирного населения' . ка\"]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.decode([list(X[522].reshape((max_sequence_length-1)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35e7ef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "не планирует передислоцировать наблюдательные пункты в идлибской зоне деэскалации , при этом турция продолжит отправлять военных и бронетехнику в этот район 'в целях защиты мирного населения' . ка,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "# Для bpe\n",
    "composition = \"не планирует передислоцировать наблюдательные пункты в идлибской зоне деэскалации , при этом турция продолжит отправлять военных и бронетехнику в этот район 'в целях защиты мирного населения' . ка\"\n",
    "next_words = 200\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = bpe.encode(composition)\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre', truncating=\"pre\")\n",
    "    token_list = token_list.reshape((1,max_sequence_length-1,1))\n",
    "    predicted = np.argmax(model2.predict(token_list), axis=-1)\n",
    "    output_character = bpe.decode([predicted])[0]\n",
    "    composition += output_character\n",
    "print(composition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c72f8c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[126, 341, 322, 6, 90, 10, 20, 156, 603, 71, 97, 109, 448]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.encode([\"приветствие затянулось на несколько \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f1ff0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "550e8289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_ordered_indices(ar):\n",
    "    d = {i:v for i,v in enumerate(ar)}\n",
    "    return sorted(d, key=d.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed6e71dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensures always sums to 1\n",
    "def normalize_softmax(ar):\n",
    "    s = sum(ar)\n",
    "    if (s!=1):\n",
    "        ar[0] += 1-s\n",
    "    return ar\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "988ff5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "как было сказано осенее с онкет пантоти и ресетиеее ес о осессон в паттеле онти портовония сакомо презедлитеви презисселе нерари соргорам сосриий,\n",
      "подономо по накраветения подоваи полодать по накогния сообщал возении,\n"
     ]
    }
   ],
   "source": [
    "composition = \"как было сказано \"\n",
    "next_words = 200\n",
    "T = 2 # токены из top-T будут случайно выбираться\n",
    "temperature = 1 # параметр сглаживания распределения выбранных токенов\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([composition])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=window_length-1, padding='pre')\n",
    "    token_list = token_list.reshape((1,window_length-1,1))\n",
    "    \n",
    "    output = model2.predict(token_list)\n",
    "    topmost_indicies = return_ordered_indices(output[0, :])[:T]\n",
    "    probs = tf.nn.softmax(output[0, topmost_indicies] /  temperature).numpy()\n",
    "    probs = normalize_softmax(probs)\n",
    "    predicted = np.random.choice(topmost_indicies, p=probs)\n",
    "#     predicted = topmost_indicies[0]\n",
    "    output_character = tokenizer.sequences_to_texts([[predicted]])[0]\n",
    "    composition += output_character\n",
    "print(composition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceebf63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_generation",
   "language": "python",
   "name": "text_generation_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
