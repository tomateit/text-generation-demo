{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "divided-thinking",
   "metadata": {},
   "source": [
    "# Генерация текста с помощью LSTM-сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-astronomy",
   "metadata": {},
   "source": [
    "Сеть способна выучить распределение символов в последовательностях\n",
    "\n",
    "\n",
    "Датасет формируем проходясь окном по текстовому корпусу, задача сети - предсказывать следующий символ на основании нескольких предыдущих.\n",
    "Данный подход можно улучшить, используя только отдельные предложения с паддингами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "paperback-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "considered-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-pakistan",
   "metadata": {},
   "source": [
    "### 0. Получение данных для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-lying",
   "metadata": {},
   "source": [
    "Для обучения используется датасет российских новостей, который я храню в локальной бд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cardiac-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql+psycopg2://postgres:mypassword@192.168.1.174/process_text_processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "unusual-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# данные в бд достаточно \"сырые\" - могут включать символы практически всех языков, куча юникода и эмодзи\n",
    "# регулярное выражение позволяет оставить только буквы русского алфавита, пунктуацию и пробелы\n",
    "data = pd.read_sql(\"\"\"\n",
    "    SELECT \"text\" FROM textdocuments WHERE \"text\" ~ '^[а-яА-ЯёЁ[:punct:]\\s]+$' OFFSET 4000 LIMIT 4000\n",
    "\"\"\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-passion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "administrative-candy",
   "metadata": {},
   "source": [
    "### 1. Вспомогательные функции:\n",
    "+ Визуализация процесса обучения\n",
    "    + Сможем посмотреть, как меняется качество с течением времени\n",
    "+ Колбек ModelCheckpoint\n",
    "    + Процесс обучения LSTM сетей достаточно длинный. Будет обидно, если из-за непредвиденного сбоя потеряется прогресс за многие часы обучения\n",
    "+ Колбек динамической подстройки размера батча и learning rate\n",
    "    + Подстраивать LR это уже стандартная практика, а я хочу ещё и размер батча менять: предположу, что большой батч позволит дать некое \"обобщённое\" представление о распределении символов, а маленький батч улучшит \"грамотность\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "conscious-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-daisy",
   "metadata": {},
   "source": [
    "### 3. Предобработка и создание датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-companion",
   "metadata": {},
   "source": [
    "Для тренировки LSTM модели я не буду менять регистр или избавляться от небуквенных символов, однако понадобится немного поработать с форматами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "exact-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Понадобится разбить тексты на предложения. Ранее я уже экспериментировал с разбиением\n",
    "corpus = [t.split(\"\\n\") for t in data.text]\n",
    "# весь текст одной \"портянкой\"\n",
    "raw_text = \" \".join(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "compound-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i+1) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i+1, c) for i, c in enumerate(chars))\n",
    "tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=None, # не ограничиваем количество токенов\n",
    "    filters='\\t', # фильтрацию не используем за исключением табов (могут попадаться)\n",
    "    lower=False, # не меняем регистр \n",
    "    split=' ', \n",
    "    char_level=True, # ставим флаг посимвольного кодирования, иначе словарь слишком велик \n",
    "    oov_token=\"<OOV>\",\n",
    "    **kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "sapphire-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "adolescent-somewhere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177274\n"
     ]
    }
   ],
   "source": [
    "print(total_words)\n",
    "total_words = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-breast",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "plain-liquid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# pad sequences \n",
    "# max_sequence_len = max([len(x) for x in input_sequences])\n",
    "max_sequence_len = 40\n",
    "print(max_sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cognitive-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = np.array(pad_sequences(input_sequences[:300000], maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictors and label\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(20)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(xs, ys, epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict_classes(token_list, verbose=0)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-characterization",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "third-federation",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "quarterly-router",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "sharing-resistance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  4082927\n",
      "Total Vocab:  88\n"
     ]
    }
   ],
   "source": [
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "interesting-english",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4082827/4082827 [00:58<00:00, 69885.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  4082827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in tqdm(range(0, n_chars - seq_length, 1)):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int.get(char, 0) for char in seq_in])\n",
    "    dataY.append(char_to_int.get(seq_out, 0))\n",
    "    \n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "adopted-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = tf.keras.utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "polished-adelaide",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "hispanic-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "# filename = \"weights-improvement-19-1.9435.hdf5\"\n",
    "# model.load_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "under-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dying-hometown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "70431/70431 [==============================] - 6263s 89ms/step - loss: 2.7826\n",
      "Epoch 2/5\n",
      "70431/70431 [==============================] - 6277s 89ms/step - loss: 2.3532\n",
      "Epoch 3/5\n",
      "70431/70431 [==============================] - 6280s 89ms/step - loss: 2.1831\n",
      "Epoch 4/5\n",
      "70431/70431 [==============================] - 6266s 89ms/step - loss: 2.0935\n",
      "Epoch 5/5\n",
      "70431/70431 [==============================] - 6261s 89ms/step - loss: 2.0356\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "confidential-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-connecticut",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "36834/40829 [==========================>...] - ETA: 25:00 - loss: 1.8301"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, epochs=5, verbose=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "corporate-kernel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lstm2layer_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lstm2layer_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('lstm2layer_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "specific-glance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  дороги, а потом должен платить как все, где тут стимул для инвестиций?» — цитирует его заявление на \"\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "revolutionary-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "composition = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "interim-intent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    composition += result\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print( \"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "promotional-uniform",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['в',\n",
       " 'к',\n",
       " 'и',\n",
       " ' ',\n",
       " 'п',\n",
       " 'о',\n",
       " 'с',\n",
       " 'т',\n",
       " 'а',\n",
       " 'в',\n",
       " 'к',\n",
       " 'и',\n",
       " ' ',\n",
       " 'п',\n",
       " 'о',\n",
       " 'с',\n",
       " 'т',\n",
       " 'а',\n",
       " 'в',\n",
       " 'к',\n",
       " 'и',\n",
       " ' ',\n",
       " 'п',\n",
       " 'о',\n",
       " 'с',\n",
       " 'т',\n",
       " 'а',\n",
       " 'в',\n",
       " 'к',\n",
       " 'и',\n",
       " ' ',\n",
       " 'п',\n",
       " 'о',\n",
       " 'с',\n",
       " 'т',\n",
       " 'а',\n",
       " 'в',\n",
       " 'к',\n",
       " 'и',\n",
       " ' ',\n",
       " 'п',\n",
       " 'о',\n",
       " 'с',\n",
       " 'т',\n",
       " 'а',\n",
       " 'в',\n",
       " 'к',\n",
       " 'и',\n",
       " ' ',\n",
       " 'п',\n",
       " 'о',\n",
       " 'с',\n",
       " 'т',\n",
       " 'а',\n",
       " 'в',\n",
       " 'к',\n",
       " 'и',\n",
       " ' ',\n",
       " 'п',\n",
       " 'о',\n",
       " 'с',\n",
       " 'т',\n",
       " 'а',\n",
       " 'в',\n",
       " 'к',\n",
       " 'и',\n",
       " ' ',\n",
       " 'п',\n",
       " 'о',\n",
       " 'с',\n",
       " 'т',\n",
       " 'а',\n",
       " 'в',\n",
       " 'к',\n",
       " 'и',\n",
       " ' ',\n",
       " 'п',\n",
       " 'о',\n",
       " 'с',\n",
       " 'т',\n",
       " 'а',\n",
       " 'в',\n",
       " 'к',\n",
       " 'и',\n",
       " ' ',\n",
       " 'п',\n",
       " 'о',\n",
       " 'с',\n",
       " 'т',\n",
       " 'а',\n",
       " 'в',\n",
       " 'к',\n",
       " 'и',\n",
       " ' ',\n",
       " 'п',\n",
       " 'о',\n",
       " 'с',\n",
       " 'т',\n",
       " 'а',\n",
       " 'в']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "varying-taiwan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' пресс-службе президента РФ России Владимир Путин. «Мо поставки продолжать продукции в состав продовольственного продажи в состав продовольственного продажи в составе продукции в составе продукции в составе продукции в составе продукции в составе проблема в составе продукции в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в составе проблема в '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-inflation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
